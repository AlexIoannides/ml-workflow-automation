{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Model Deployment\n",
    "\n",
    "The purpose of this notebook is to demonstrate how the model yielded by `titanic-ml.ipynb` can be deployed as a RESTful service on Kubernetes.\n",
    "\n",
    "Required steps:\n",
    "\n",
    "1. inject model into a Flask service:\n",
    "    - create 'deploy' directory;\n",
    "    - 'download' model locally;\n",
    "    - 'download' Flask model wrapper;\n",
    "    - use environment variable to pass model name to Flask service;\n",
    "2. inject Flask service into Docker container:\n",
    "    - build Docker image;\n",
    "    - push image to registry;\n",
    "3. deploy end-to-end service to Kubernetes using a Helm chart:\n",
    "    - download the appropriate Helm chart;\n",
    "    - set the appropriate parameter values for service naming, clusters, etc.; and,\n",
    "    - deploy using Helm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mandatory Deployment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'models/titanic-ml-2019-02-11T17:48:28.joblib'\n",
    "SERVICE_NAME = 'titanic'\n",
    "API_VERSION = '1'\n",
    "DOCKER_IMAGE_REGISTRY = 'alexioannides'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Model to `py-flask-ml-service`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = os.popen(f'cp {MODEL} deploy/py-flask-ml-service/model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy `Pipfile` ?\n",
    "\n",
    "- how to best manage the needs of the service with the needs of the notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Service Parameters to `.env` File in `py-flask-ml-service`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('deploy/py-flask-ml-service/.env', 'w') as file:\n",
    "    file.writelines(f'SERVICE_NAME={SERVICE_NAME}\\n')\n",
    "    file.writelines(f'API_VERSION={API_VERSION}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Docker Image\n",
    "\n",
    "- Note, we will need a build server (e.g. Travis) to build and push."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tag = f'{DOCKER_IMAGE_REGISTRY}/{SERVICE_NAME}:latest'\n",
    "\n",
    "docker_client = docker.from_env()\n",
    "image, build_log = docker_client.images.build(\n",
    "    path='deploy/py-flask-ml-service', tag=image_tag, rm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push Image to DockerHub\n",
    "\n",
    "- Will need to use `docker_client.login()` on a build server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = docker_client.images.push(image_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to Kubernetes using Helm Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   xrayed-olm\n",
      "LAST DEPLOYED: Wed Feb 13 00:11:58 2019\n",
      "NAMESPACE: default\n",
      "STATUS: DEPLOYED\n",
      "\n",
      "RESOURCES:\n",
      "==> v1/Namespace\n",
      "NAME     STATUS  AGE\n",
      "titanic  Active  0s\n",
      "\n",
      "==> v1/Service\n",
      "NAME                            TYPE          CLUSTER-IP     EXTERNAL-IP  PORT(S)         AGE\n",
      "titanic-survival-prediction-lb  LoadBalancer  10.98.211.202  <pending>    5000:30438/TCP  0s\n",
      "\n",
      "==> v1/ReplicationController\n",
      "NAME                            DESIRED  CURRENT  READY  AGE\n",
      "titanic-survival-prediction-rc  2        2        0      0s\n",
      "\n",
      "==> v1/Pod(related)\n",
      "NAME                                  READY  STATUS             RESTARTS  AGE\n",
      "titanic-survival-prediction-rc-bsr8f  0/1    ContainerCreating  0         0s\n",
      "titanic-survival-prediction-rc-x74ph  0/1    ContainerCreating  0         0s\n",
      "\n",
      "\n",
      "NOTES:\n",
      "Thank you for installing helm-ml-score-app.\n",
      "\n",
      "Your release is named xrayed-olm.\n",
      "\n",
      "To learn more about the release, try:\n",
      "\n",
      "  $ helm status xrayed-olm\n",
      "  $ helm get xrayed-olm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!helm install deploy/helm-ml-prediction-app \\\n",
    "    --set app.name=\"$SERVICE_NAME-survival-prediction\" \\\n",
    "    --set app.namespace=\"$SERVICE_NAME\" \\\n",
    "    --set app.image=\"$image_tag\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Prediction Service\n",
    "\n",
    "We will assume that Minikube is being used for local testing, in which case we will need to ask it for the URL of its 'virtual load balancer' used for our service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-------------|--------------------------------|-----------------------------|\n",
      "|  NAMESPACE  |              NAME              |             URL             |\n",
      "|-------------|--------------------------------|-----------------------------|\n",
      "| default     | kubernetes                     | No node port                |\n",
      "| kube-system | kube-dns                       | No node port                |\n",
      "| kube-system | kubernetes-dashboard           | No node port                |\n",
      "| kube-system | tiller-deploy                  | No node port                |\n",
      "| titanic     | titanic-survival-prediction-lb | http://192.168.99.114:30438 |\n",
      "|-------------|--------------------------------|-----------------------------|\n"
     ]
    }
   ],
   "source": [
    "!minikube service list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then take the appropriate URL from the above and test our titanic prediction service!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"prediction\":[1]}\n"
     ]
    }
   ],
   "source": [
    "!curl http://192.168.99.114:30438/titanic/v1/predict \\\n",
    "        --request POST\\\n",
    "        --header 'Content-Type: application/json' \\\n",
    "        --data '{\"Pclass\": [1], \"Sex\": [\"male\"], \"Age\": [32], \"SibSp\": [1],                      \"Parch\": [0], \"Fare\": [100], \"Embarked\": [\"S\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
