{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Model Deployment\n",
    "\n",
    "The purpose of this notebook is to demonstrate how the model yielded by `titanic-ml.ipynb` can be deployed as a RESTful service on Kubernetes.\n",
    "\n",
    "Required steps:\n",
    "\n",
    "1. inject model into a Flask service:\n",
    "    - create 'deploy' directory;\n",
    "    - 'download' model locally;\n",
    "    - 'download' Flask model wrapper;\n",
    "    - use environment variable to pass model name to Flask service;\n",
    "2. inject Flask service into Docker container:\n",
    "    - build Docker image;\n",
    "    - push image to registry;\n",
    "3. deploy end-to-end service to Kubernetes using a Helm chart:\n",
    "    - download the appropriate Helm chart;\n",
    "    - set the appropriate parameter values for service naming, clusters, etc.; and,\n",
    "    - deploy using Helm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_REPOSITORY = '../models'\n",
    "SERVICE_NAME = 'titanic'\n",
    "API_VERSION = '1'\n",
    "DOCKER_IMAGE_REGISTRY = 'alexioannides'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Model to Flask Service Source Code Directory\n",
    "\n",
    "Which is `py-sklearn-flask-ml-service` within the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titanic-ml-2019-02-11T17:48:28.joblib --> deploy/py-sklearn-flask-ml-service/model.joblib\n"
     ]
    }
   ],
   "source": [
    "latest_model = sorted(os.listdir(MODEL_REPOSITORY))[-1]\n",
    "os.popen(f'cp {latest_model} deploy/py-sklearn-flask-ml-service/model.joblib')\n",
    "print(f'{latest_model} --> deploy/py-sklearn-flask-ml-service/model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Service Configuration Parameters\n",
    "\n",
    "We use a `.env` file in `py-sklearn-flask-ml-service` that will be copied to the Docker image, automatically loaded by Pipenv and used by the Flask service described in `py-sklearn-flask-ml-service` to define the REST API endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('py-sklearn-flask-ml-service/.env', 'w') as file:\n",
    "    file.writelines(f'SERVICE_NAME={SERVICE_NAME}\\n')\n",
    "    file.writelines(f'API_VERSION={API_VERSION}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the contents of the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVICE_NAME=titanic\n",
      "API_VERSION=1\n"
     ]
    }
   ],
   "source": [
    "!cat py-sklearn-flask-ml-service/.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Docker Image\n",
    "\n",
    "Ensure that there is a Docker daemon up-and-runnning.\n",
    "\n",
    "> Note, in production we will need a build server (e.g. Travis CI) to build images and push to registries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stream': 'Step 1/7 : FROM python:3.7-slim'}\n",
      "{'stream': '\\n'}\n",
      "{'stream': ' ---> 12c44ed85032\\n'}\n",
      "{'stream': 'Step 2/7 : WORKDIR /usr/src/app'}\n",
      "{'stream': '\\n'}\n",
      "{'stream': ' ---> Using cache\\n'}\n",
      "{'stream': ' ---> 987983ee8d98\\n'}\n",
      "{'stream': 'Step 3/7 : COPY . .'}\n",
      "{'stream': '\\n'}\n",
      "{'stream': ' ---> Using cache\\n'}\n",
      "{'stream': ' ---> 0ed602e98f4e\\n'}\n",
      "{'stream': 'Step 4/7 : EXPOSE 5000'}\n",
      "{'stream': '\\n'}\n",
      "{'stream': ' ---> Using cache\\n'}\n",
      "{'stream': ' ---> c86b62a88f5c\\n'}\n",
      "{'stream': 'Step 5/7 : RUN pip install pipenv'}\n",
      "{'stream': '\\n'}\n",
      "{'stream': ' ---> Using cache\\n'}\n",
      "{'stream': ' ---> 48a84b9efda6\\n'}\n",
      "{'stream': 'Step 6/7 : RUN pipenv install'}\n",
      "{'stream': '\\n'}\n",
      "{'stream': ' ---> Using cache\\n'}\n",
      "{'stream': ' ---> 5aa23904911f\\n'}\n",
      "{'stream': 'Step 7/7 : ENTRYPOINT [\"pipenv\", \"run\", \"python\", \"api.py\"]'}\n",
      "{'stream': '\\n'}\n",
      "{'stream': ' ---> Using cache\\n'}\n",
      "{'stream': ' ---> 7ecf88006784\\n'}\n",
      "{'aux': {'ID': 'sha256:7ecf88006784076c94f7e1e69f5739cef092c7f83b23b9117145550533dfddb7'}}\n",
      "{'stream': 'Successfully built 7ecf88006784\\n'}\n",
      "{'stream': 'Successfully tagged alexioannides/titanic:latest\\n'}\n"
     ]
    }
   ],
   "source": [
    "image_tag = f'{DOCKER_IMAGE_REGISTRY}/{SERVICE_NAME}:latest'\n",
    "\n",
    "docker_client = docker.from_env()\n",
    "image, build_log = docker_client.images.build(\n",
    "    path='py-sklearn-flask-ml-service', tag=image_tag, rm=True)\n",
    "\n",
    "docker_client.images.prune()\n",
    "\n",
    "for line in build_log:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push Image to DockerHub\n",
    "\n",
    "- Will need to use `docker_client.login()` on a build server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status\":\"The push refers to repository [docker.io/alexioannides/titanic]\"}\n",
      "{\"status\":\"Preparing\",\"progressDetail\":{},\"id\":\"2b117c418425\"}\n",
      "{\"status\":\"Preparing\",\"progressDetail\":{},\"id\":\"a78c9ad3c595\"}\n",
      "{\"status\":\"Preparing\",\"progressDetail\":{},\"id\":\"9449b123dded\"}\n",
      "{\"status\":\"Preparing\",\"progressDetail\":{},\"id\":\"f49f129dd410\"}\n",
      "{\"status\":\"Preparing\",\"progressDetail\":{},\"id\":\"be1cb44a8fa5\"}\n",
      "{\"status\":\"Preparing\",\"progressDetail\":{},\"id\":\"94a02319f331\"}\n",
      "{\"status\":\"Preparing\",\"progressDetail\":{},\"id\":\"c22eb8781541\"}\n",
      "{\"status\":\"Preparing\",\"progressDetail\":{},\"id\":\"622d7e308e90\"}\n",
      "{\"status\":\"Preparing\",\"progressDetail\":{},\"id\":\"0a07e81f5da3\"}\n",
      "{\"status\":\"Waiting\",\"progressDetail\":{},\"id\":\"94a02319f331\"}\n",
      "{\"status\":\"Waiting\",\"progressDetail\":{},\"id\":\"c22eb8781541\"}\n",
      "{\"status\":\"Waiting\",\"progressDetail\":{},\"id\":\"622d7e308e90\"}\n",
      "{\"status\":\"Waiting\",\"progressDetail\":{},\"id\":\"0a07e81f5da3\"}\n",
      "{\"status\":\"Layer already exists\",\"progressDetail\":{},\"id\":\"2b117c418425\"}\n",
      "{\"status\":\"Layer already exists\",\"progressDetail\":{},\"id\":\"a78c9ad3c595\"}\n",
      "{\"status\":\"Layer already exists\",\"progressDetail\":{},\"id\":\"9449b123dded\"}\n",
      "{\"status\":\"Layer already exists\",\"progressDetail\":{},\"id\":\"be1cb44a8fa5\"}\n",
      "{\"status\":\"Layer already exists\",\"progressDetail\":{},\"id\":\"f49f129dd410\"}\n",
      "{\"status\":\"Layer already exists\",\"progressDetail\":{},\"id\":\"94a02319f331\"}\n",
      "{\"status\":\"Layer already exists\",\"progressDetail\":{},\"id\":\"622d7e308e90\"}\n",
      "{\"status\":\"Layer already exists\",\"progressDetail\":{},\"id\":\"c22eb8781541\"}\n",
      "{\"status\":\"Layer already exists\",\"progressDetail\":{},\"id\":\"0a07e81f5da3\"}\n",
      "{\"status\":\"latest: digest: sha256:89dadbdb75a1efd9914592daaa6ac5eccbb8c9c1af184a05a3d5ca08b4ef25f3 size: 2213\"}\n",
      "{\"progressDetail\":{},\"aux\":{\"Tag\":\"latest\",\"Digest\":\"sha256:89dadbdb75a1efd9914592daaa6ac5eccbb8c9c1af184a05a3d5ca08b4ef25f3\",\"Size\":2213}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "push_log = docker_client.images.push(image_tag)\n",
    "print(push_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to Kubernetes using Helm Charts\n",
    "\n",
    "We use the Helm chart located at `k8s-helm-ml-prediction-app` and discussed [here](https://github.com/AlexIoannides/kubernetes-ml-ops) to deploy our ML REST API as a fully-managed, self-healing and load balanced service on Kubernetes.\n",
    "\n",
    "Note, ensure that `kubectl config current-context` is set the Kubernetes cluster we wish to deploy to and that this cluster has an operational Helm Tiller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   newbie-stingray\n",
      "LAST DEPLOYED: Wed Feb 13 01:21:24 2019\n",
      "NAMESPACE: default\n",
      "STATUS: DEPLOYED\n",
      "\n",
      "RESOURCES:\n",
      "==> v1/Pod(related)\n",
      "NAME                                  READY  STATUS             RESTARTS  AGE\n",
      "titanic-survival-prediction-rc-4tqd2  0/1    ContainerCreating  0         0s\n",
      "titanic-survival-prediction-rc-t9sv9  0/1    ContainerCreating  0         0s\n",
      "\n",
      "==> v1/Namespace\n",
      "NAME     STATUS  AGE\n",
      "titanic  Active  1s\n",
      "\n",
      "==> v1/Service\n",
      "NAME                            TYPE          CLUSTER-IP     EXTERNAL-IP  PORT(S)         AGE\n",
      "titanic-survival-prediction-lb  LoadBalancer  10.108.72.116  <pending>    5000:31187/TCP  0s\n",
      "\n",
      "==> v1/ReplicationController\n",
      "NAME                            DESIRED  CURRENT  READY  AGE\n",
      "titanic-survival-prediction-rc  2        2        0      0s\n",
      "\n",
      "\n",
      "NOTES:\n",
      "Thank you for installing helm-ml-score-app.\n",
      "\n",
      "Your release is named newbie-stingray.\n",
      "\n",
      "To learn more about the release, try:\n",
      "\n",
      "  $ helm status newbie-stingray\n",
      "  $ helm get newbie-stingray\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!helm install k8s-helm-ml-prediction-app \\\n",
    "    --set app.name=\"$SERVICE_NAME-survival-prediction\" \\\n",
    "    --set app.namespace=\"$SERVICE_NAME\" \\\n",
    "    --set app.image=\"$image_tag\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Prediction Service\n",
    "\n",
    "We will assume that Minikube is being used for local testing, in which case we will need to ask it for the URL of its 'virtual load balancer' used for our service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-------------|--------------------------------|-----------------------------|\n",
      "|  NAMESPACE  |              NAME              |             URL             |\n",
      "|-------------|--------------------------------|-----------------------------|\n",
      "| default     | kubernetes                     | No node port                |\n",
      "| kube-system | kube-dns                       | No node port                |\n",
      "| kube-system | kubernetes-dashboard           | No node port                |\n",
      "| kube-system | tiller-deploy                  | No node port                |\n",
      "| titanic     | titanic-survival-prediction-lb | http://192.168.99.114:30438 |\n",
      "|-------------|--------------------------------|-----------------------------|\n"
     ]
    }
   ],
   "source": [
    "!minikube service list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then take the appropriate URL from the above and test our titanic prediction service!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"prediction\":[1]}\n"
     ]
    }
   ],
   "source": [
    "!curl http://192.168.99.114:30438/titanic/v1/predict \\\n",
    "        --request POST\\\n",
    "        --header 'Content-Type: application/json' \\\n",
    "        --data '{\"Pclass\": [1], \"Sex\": [\"male\"], \"Age\": [32], \"SibSp\": [1],                      \"Parch\": [0], \"Fare\": [100], \"Embarked\": [\"S\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
